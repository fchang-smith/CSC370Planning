{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fchang-smith/CSC370Planning/blob/main/Class_Activity_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credit: This lab was inspired by https://github.com/aamini/introtodeeplearning/ from MIT 6.S191 Intro to Deep Learning, borrowed parts of the code and model architecture from https://nextjournal.com/gkoehler/pytorch-mnist, and was compiled and modified by Feiran Chang."
      ],
      "metadata": {
        "id": "Yo0xJTEfKhXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: MNIST Digit Classification**\n",
        "In the first portion of this lab, we will build and train a convolutional neural network (CNN) for classification of handwritten digits from the famous MNIST dataset. The MNIST dataset consists of 60,000 training images and 10,000 test images. Our classes are the digits 0-9.\n",
        "\n",
        "First, let's import the relevant packages we'll need for this lab."
      ],
      "metadata": {
        "id": "PjpNT-W6rAtn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ6fguNiqjml"
      },
      "outputs": [],
      "source": [
        "# Import pyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check that we are using a GPU."
      ],
      "metadata": {
        "id": "3VStrnPB07--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that we are using a GPU, if not switch runtimes\n",
        "# The program should print \"cuda\"\n",
        "#   using Runtime > Change Runtime Type > GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "assert(device == 'cuda')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "UcZFT4c506z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1 MNIST dataset\n",
        "Let's download and load the dataset using PyTorch's specific functions, such as Dataset and DataLoader, to handle loading data and labels. We can also define the batch size during this step.\n"
      ],
      "metadata": {
        "id": "PQQSfpq0rXp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "  datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=transforms.Compose([\n",
        "                               transforms.ToTensor()\n",
        "                             ])),\n",
        "  batch_size=64, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=transforms.Compose([\n",
        "                               transforms.ToTensor(),\n",
        "                             ])),\n",
        "  batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "# Access the first batch of training data\n",
        "example_iter = iter(train_loader)\n",
        "example_images, example_labels = next(example_iter)\n",
        "\n",
        "# You can print out the shape of a tensor by using variable.shape\n",
        "print(f\"example_images: {example_images.shape}\")\n",
        "print(f\"example_labels: {example_labels.shape}\")"
      ],
      "metadata": {
        "id": "ywfVlml4q_dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our training set is made up of 28x28 grayscale images of handwritten digits.\n",
        "\n",
        "Let's visualize what some of these images and their corresponding training labels look like."
      ],
      "metadata": {
        "id": "qXLHUuXCu4Qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "random_inds = np.random.choice(60000,36)\n",
        "for i in range(36):\n",
        "  plt.subplot(6,6,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  image_ind = random_inds[i]\n",
        "  plt.imshow(np.squeeze(example_images[i]), cmap=plt.cm.binary)\n",
        "  plt.xlabel(example_labels[i].item())"
      ],
      "metadata": {
        "id": "XuQ9kRi_u5WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Neural Network for Handwritten Digit Classification\n",
        "We'll first build a simple neural network consisting of two fully connected layers and apply this to the digit classification task. Our network will ultimately output a probability distribution over the 10 digit classes (0-9). This first architecture we will be building is depicted below:\n",
        "![alt_text](https://raw.githubusercontent.com/aamini/introtodeeplearning/master/lab2/img/mnist_2layers_arch.png \"CNN Architecture for MNIST Classification\")\n"
      ],
      "metadata": {
        "id": "0yVqLrLsxw4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fully connected neural network architecture\n",
        "To define the architecture of this first fully connected neural network, we'll define the model using the `nn.Linear` layer. Note how we first use a `nn.Flatten` layer, which flattens the input so that it can be fed into the model.\n",
        "\n",
        "In this next block, you'll define the fully connected layers of this simple work."
      ],
      "metadata": {
        "id": "WEfs9sv3x9Ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class mlp_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(mlp_model, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.mlp1 = nn.Linear(784, 128)  # Adjust input size based on your data\n",
        "        self.mlp2 = nn.Linear(128, 10)  # Adjust output size based on your task (e.g., number of classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.mlp1(x))\n",
        "        x = F.softmax(self.mlp2(x), dim=1)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model_1 = mlp_model()\n"
      ],
      "metadata": {
        "id": "tFirGLB5x1uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a step back and think about the network we've just created. The first layer in this network, `nn.Flatten`, transforms the format of the images from a 2d-array (28 x 28 pixels), to a 1d-array of 28 * 28 = 784 pixels. You can think of this layer as unstacking rows of pixels in the image and lining them up. There are no learned parameters in this layer; it only reformats the data.\n",
        "\n",
        "After the pixels are flattened, the network consists of a sequence of two `nn.Linear` layers. These are fully-connected neural layers. The first `Linear` layer has 128 nodes (or neurons). The second (and last) layer (which you've defined!) should return an array of probability scores that sum to 1. Each node contains a score that indicates the probability that the current image belongs to one of the handwritten digit classes.\n",
        "\n",
        "That defines our fully connected model!"
      ],
      "metadata": {
        "id": "4SgTWTKc0LRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model\n",
        "\n",
        "We're now ready to train our model, which will involve feeding the training data (`train_images` and `train_labels`) into the model, and then asking it to learn the associations between images and labels. We'll also need to define the number of epochs, or iterations over the MNIST dataset, to use during training.\n",
        "\n",
        "We'll use a stochastic gradient descent (SGD) optimizer initialized with a learning rate of 0.1. Since we are performing a categorical classification task, we'll want to use the cross entropy loss."
      ],
      "metadata": {
        "id": "J7V9fNxw0cHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up key parameters\n",
        "num_epoch = 5\n",
        "learning_rate = 0.1\n",
        "momentum = 0.5\n",
        "log_interval = 100\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_1.parameters(), lr=learning_rate, momentum = momentum)\n",
        "\n",
        "# Using array to keep track of the loss\n",
        "loss_array = []\n",
        "\n",
        "# Training loop\n",
        "# During each epoch, the model is trained on the entire dataset\n",
        "for epoch in range(num_epoch):\n",
        "  model_1.train() # Set up trainning mode\n",
        "  train_iter = iter(train_loader)\n",
        "\n",
        "  # loop over all batches in the dataloader\n",
        "  for train_idx, (train_images, train_labels) in enumerate(train_iter):\n",
        "    pred = model_1.forward(train_images) # Use the model to predict the label\n",
        "    loss = loss_function(pred, train_labels) # Compute loss\n",
        "    loss.backward() # Backpropogation\n",
        "    optimizer.step() # Modify the learning rate based on the optimizer's algorithm\n",
        "    optimizer.zero_grad()\n",
        "    loss_array.append(loss.item())\n",
        "\n",
        "    # Print out metrics very 100 batches\n",
        "    if train_idx % log_interval == 0: # Remind: log_interval = 100\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch+1, train_idx * len(train_images), len(train_loader.dataset),\n",
        "        100. * train_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "AcAhbETz7ZK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate accuracy on the test dataset\n",
        "\n",
        "Now that we've trained the model, we can ask it to make predictions about a test set that it hasn't seen before. In this example, the `test_images` array comprises our test dataset. To evaluate accuracy, we can check to see if the model's predictions match the labels from the `test_labels` array.\n"
      ],
      "metadata": {
        "id": "184UQxSrEsGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.eval() # Set up the evaluation mode\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "  # Loop over the test dataset only once\n",
        "  for test_images, test_labels in test_loader:\n",
        "    pred = model_1(test_images)\n",
        "    test_loss += F.nll_loss(pred, test_labels, size_average=False).item()\n",
        "    pred = pred.data.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(test_labels.data.view_as(pred)).sum() # Compare labels and predictions\n",
        "test_loss /= len(test_loader.dataset)\n",
        "print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "  test_loss, correct, len(test_loader.dataset),\n",
        "  100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "KU7_97XDEyQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the highest accuracy you can achieve with this first fully connected model? Since the handwritten digit classification task is pretty straightforward, you may be wondering how we can do better..."
      ],
      "metadata": {
        "id": "Hwx_JItbPmt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Convolutional Neural Network (CNN) for handwritten digit classification\n",
        "\n",
        "Convolutional neural networks (CNNs) are particularly well-suited for a variety of tasks in computer vision, and have achieved near-perfect accuracies on the MNIST dataset. We will now build a CNN composed of two convolutional layers and pooling layers, followed by two fully connected layers, and ultimately output a probability distribution over the 10 digit classes (0-9). The CNN we will be building is depicted below:\n",
        "\n",
        "![CNN Architecture for MNIST Classification](https://raw.githubusercontent.com/fchang-smith/CSC370Planning/main/cnn_structure.jpg \"CNN Architecture for MNIST Classification\")\n"
      ],
      "metadata": {
        "id": "-sHZADHyPoJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the CNN model\n",
        "\n",
        "We'll use the same training and test datasets as before, and proceed similarly as our fully connected network to define and train our new CNN model. To do this we will explore two layers we have not encountered before: you can use  `nn.Conv2d` to define convolutional layers and `nn.MaxPool2d` to define the pooling layers. Use the parameters shown in the network architecture above to define these layers and build the CNN model."
      ],
      "metadata": {
        "id": "I3qATRqiP4Za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class cnn_model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(cnn_model, self).__init__()\n",
        "    # TODO: Define the first convolutional layer\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=0)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=0)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.mlp1 = nn.Linear(320, 50)\n",
        "    self.mlp2 = nn.Linear(50, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool2(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.mlp1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.mlp2(x)\n",
        "    x = F.log_softmax(x)\n",
        "    return x\n",
        "    # you can also write it as:\n",
        "    # x = self.pool1(F.relu(self.con1(x)))\n",
        "    # x = self.pool2(F.relu(self.con2(x)))\n",
        "    # x = self.flatten(x)\n",
        "    # x = F.relu(self.mlp1(x))\n",
        "    # x = F.log_softmax(self.mlo2(x))\n",
        "\n",
        "# Create an instance of the CNN model\n",
        "model_2 = cnn_model()\n",
        "\n",
        "# Print the summary of the layers in the model\n",
        "print(model_2)"
      ],
      "metadata": {
        "id": "Vw12ZHCaP17o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and test the CNN model\n",
        "\n",
        "Now, as before, we can define the loss function, optimizer, and calculate accuracy.\n",
        "Basically, the only difference between this training loop and the multiple percepton (mlp) trainning loop is that model_1 is substituted with model_2.\n",
        "\n"
      ],
      "metadata": {
        "id": "PsXpXIdgWS_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Set up key parameters\n",
        "num_epoch = 5\n",
        "learning_rate = 0.1\n",
        "momentum = 0.5\n",
        "log_interval = 100\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_2.parameters(), lr=learning_rate, momentum = momentum)\n",
        "\n",
        "# Using array to keep track of the loss\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "\n",
        "# Training loop\n",
        "# During each epoch, the model is trained on the entire dataset\n",
        "for epoch in range(num_epoch):\n",
        "  model_2.train() # Set up trainning mode\n",
        "  train_iter = iter(train_loader)\n",
        "  # loop over all batches in the dataloader\n",
        "  for train_idx, (train_images, train_labels) in enumerate(train_iter):\n",
        "    pred = model_2.forward(train_images) # Use the model to predict the label\n",
        "    loss = loss_function(pred, train_labels) # Compute loss\n",
        "    loss.backward() # Backpropogation\n",
        "    optimizer.step() # Modify the learning rate based on the optimizer's algorithm\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if train_idx % log_interval == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch+1, train_idx * len(train_images), len(train_loader.dataset),\n",
        "        100. * train_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append((train_idx*64) + ((epoch)*len(train_loader.dataset)))"
      ],
      "metadata": {
        "id": "i8NoefnVUiNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, let's test the model and calculate the accuracy."
      ],
      "metadata": {
        "id": "xDsFZ5vU8YA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "  for test_images, test_labels in test_loader:\n",
        "    pred = model_2(test_images)\n",
        "    test_loss += F.nll_loss(pred, test_labels, size_average=False).item()\n",
        "    pred = pred.data.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(test_labels.data.view_as(pred)).sum()\n",
        "test_loss /= len(test_loader.dataset)\n",
        "print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "  test_loss, correct, len(test_loader.dataset),\n",
        "  100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "i9pQUIZPtqPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize the loss descending process."
      ],
      "metadata": {
        "id": "veQBpCYr8emj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(train_counter, train_losses, color='blue')\n",
        "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
        "plt.xlabel('number of training examples seen')\n",
        "plt.ylabel('negative log likelihood loss')"
      ],
      "metadata": {
        "id": "PNjSvCl5zlAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A failed example\n",
        "The model below is too complex that it does not perform well. You can try it yourself by uncommenting everything in the cell below.\n",
        "\n",
        "![alt_text](https://raw.githubusercontent.com/aamini/introtodeeplearning/master/lab2/img/convnet_fig.png \"CNN Architecture for MNIST Classification\")"
      ],
      "metadata": {
        "id": "WaAf1wEg8cdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class cnn_model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(cnn_model, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=1, padding=0)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=0)\n",
        "    self.conv2 = nn.Conv2d(24, 36, kernel_size=(3, 3), stride=1, padding=0)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=0)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.mlp1 = nn.Linear(36 * 5 * 5, 128)\n",
        "    self.mlp2 = nn.Linear(128, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x)) # output size: 24 * 26 * 26\n",
        "    x = self.pool1(x) # output size: 24 * 13 * 13\n",
        "    x = F.relu(self.conv2(x)) # output size: 36 * 11 * 11\n",
        "    x = self.pool2(x) # output size: 36 * 5 * 5\n",
        "    x = self.flatten(x) # output size: 900\n",
        "    x = F.relu(self.mlp1(x))\n",
        "    x = self.mlp2(x)\n",
        "    return F.softmax(x, dim=0)"
      ],
      "metadata": {
        "id": "4cIV7NiLxBg2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}